# PokerAI: 基于自博弈的单挑限注德州扑克AI训练系统

## 1. 项目概述

本项目旨在创建一个功能完备、规则严谨的单挑限注德州扑克（Heads-Up Limit Hold'em）环境，并利用这个环境，通过AI之间的自我博弈（Self-Play），从零开始训练出具备高水平决策能力的扑克人工智能。

项目的核心哲学借鉴了DeepMind在AlphaGo和AlphaZero上的成功经验：即AI的能力并非源于人类知识的硬编码，而是通过海量的、自动化的自我对弈，从最基本的规则中学习和发现最优策略。

## 2. 核心特性

- **规则完备的环境**: 严格实现了单挑限注德州扑克的全部核心规则。
- **比赛模式**: 支持筹码在牌局间持续累计的“比赛模式”，真实模拟AI间的长期对抗。
- **模块化设计**: 环境、智能体（Agent）、数据处理高度解耦，易于扩展和维护。
- **双模数据记录**: 自动将每一手牌局记录为人类可读的日志（`.txt`）和可供AI训练的向量化数据（`.csv`）。
- **标准化AI接口**: 定义了清晰的Agent基类，任何遵循该接口的AI都可无缝接入系统。

## 3. 项目结构

D:.
│  start_training.py
│  README.md
│
├─agents
│  │  base_agent.py
│  │  raise_agent.py
│  │
│  └─__pycache__
│          base_agent.cpython-312.pyc
│          raise_agent.cpython-312.pyc
│
├─docs
│      1.草稿纸.md
│      Heads-up limit hold’em poker is solved.pdf
│
└─logs
    └─20250830_180029
            gamelog.txt
            training_data.csv

## 4. 游戏规则与系统运行逻辑

### 4.1. 游戏规则: 单挑限注德州扑克 (Heads-Up Limit Hold'em)

- **核心参数**:
  - **玩家数量**: 2
  - **初始筹码**: 200
  - **小盲注 (SB)**: 1
  - **大盲注 (BB)**: 2
  - **小注 (Small Bet)**: 2 (用于翻牌前和翻牌圈)
  - **大注 (Big Bet)**: 4 (用于转牌和河牌圈)

- **行动顺序**:
  - **翻牌前 (Pre-flop)**: 庄家(Button/SB) **先行动**。
  - **翻牌后 (Flop, Turn, River)**: 大盲注(BB) **先行动**。

- **下注结构**:
  - 每轮下注最多包含**1次下注(Bet)和2次加注(Raise)**，即总共最多3次主动提升注额的机会。

- **游戏模式**:
  - 采用“比赛模式”，玩家的筹码在牌局之间**持续累计**，直到一方筹码归零，整场比赛结束。

### 4.2. 系统运行逻辑

本系统遵循经典的强化学习“智能体-环境”交互循环：

1.  **初始化**: `run_game.py` 脚本初始化 `PokerEnv` 环境和两个 `Agent` 实例。
2.  **开始牌局**: `run_game.py` 调用 `env.reset()`。环境会轮换庄家位置，为玩家发牌，处理盲注，并返回初始的游戏状态(`state`)。
3.  **决策循环**:
    a. `run_game.py` 从 `state` 中获取当前行动玩家(`current_player`)和其所有合法动作(`legal_actions`)。
    b. 对应的 `Agent` 调用其 `.act(state, legal_actions)` 方法，从合法动作中选择一个动作返回。
    c. `run_game.py` 将 `Agent` 选择的动作传入 `env.step(action)`。
    d. `PokerEnv` 环境根据动作更新内部状态（筹码、底池、公共牌等），记录历史，并返回新的 `state`。
    e. 循环 a-d，直到一手牌结束 (`done == True`)。
4.  **数据记录**:
    a. 牌局结束后，`run_game.py` 调用 `encoder.py` 中的函数，将最终的 `state` 从两位玩家的视角，分别编码成两个标准化的**PSV向量**。
    b. `run_game.py` 调用 `logger.py` 中的 `GameLogger`，将牌局信息同时记录到 `gamelog.txt` 和 `training_data.csv` 文件中。
5.  **循环牌局**: 重复步骤 2-4，直到达到设定的最大局数或有玩家破产。

## 5. 未来AI开发计划

我们的最终目标是创建一个强大的扑克AI。为此，我们规划了以下开发路径：

### 阶段一: 坚实的数据基础 - PSV (已设计完成)

我们已经设计了**“扑克状态向量”（Poker State Vector, PSV）**，这是一个244维的标准化数字向量。它是连接游戏世界和AI大脑的桥梁，能够将一局牌局中所有关键信息（手牌、公共牌、位置、筹码、完整历史动作序列）无损地提供给神经网络。

### 阶段二: 监督学习 - 构建一个基础模型

我们的第一个“智能”AI将通过监督学习（Supervised Learning）构建，作为一个行为克隆的基准。

1.  **数据生成**: 使用当前的 `RaiseAgent` 或 `RandomAgent` 进行大量自博弈，生成海量的 `training_data.csv` 文件。
2.  **模型构建**:
    - **输入**: 244维的PSV。
    - **网络结构**: 一个标准的多层全连接神经网络。
    - **输出 (双头)**:
        - **策略头 (Policy Head)**: 输出一个概率分布，预测在此状态下，采取每个合法动作（fold, call, raise）的概率。
        - **价值头 (Value Head)**: 输出一个单一数值，预测在此状态下，最终期望能赢/输掉多少筹- **训练**: 使用 `training_data.csv` 中的PSV作为输入，将历史中真实发生的动作作为策略头的标签，将最终的输赢结果作为价值头的标签，进行训练。

### 阶段三: 强化学习 - 实现真正的自我进化

在拥有一个基础模型后，我们将进入真正的强化学习阶段，让AI通过自我博弈实现超越。

1.  **自博弈循环**:
    a. 让当前最强的AI模型（初始为阶段二的模型）进行自博弈。
    b. AI的决策不再是随机的，而是基于其神经网络的预测（可能会结合MCTS蒙特卡洛树搜索来优化决策）。
    c. 将这些更高质量的对局数据，再次编码为PSV和对局结果。
2.  **模型再训练**:
    a. 使用新生成的、更高质量的数据来“微调”或“重新训练”神经网络。
    b. 训练的目标是让策略头更接近MCTS的搜索结果，让价值头更准确地预测最终的真实输赢。
3.  **模型迭代**: 将训练出的新模型作为下一代AI，重复步骤1。

通过不断重复这个“**对弈 -> 学习 -> 迭代**”的循环，AI将逐渐摆脱初始数据的束缚，发现人类玩家可能从未想到的、更优的扑克策略，最终达到我们的目标。