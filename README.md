# PokerAI: 基于自博弈的HULHE AI训练平台

**版本**: 1.1
**状态**: 核心环境设计完成 (包含高级特性)

## 1. 项目概述

本项目旨在创建一个功能完备、规则严谨的**单挑限注德州扑克 (Heads-Up Limit Hold'em, HULHE)** 训练环境。该环境将作为未来所有AI训练的“官方赛场”，通过AI之间的海量自我博弈，为训练出具备高水平决策能力和强大泛化能力的扑克人工智能提供坚实的基础。

项目的核心哲学是**模块化**、**数据驱动**与**泛化训练**:
- **模块化**: 环境、智能体、工具和训练脚本严格分离，确保代码的可维护性和可扩展性。
- **数据驱动**: 整个系统被设计为一个高效的数据生成管道，能够将模拟的牌局转化为信息完备的、可供机器学习模型使用的向量化数据。
- **泛化训练**: 环境内置了随机化初始筹码的功能，旨在通过丰富数据多样性，从一开始就培养AI对不同筹码深度的适应能力。

## 2. 最终项目文件结构

```
PokerAI/
|
├── HULHE_env/              # 存放“扑克世界”的核心逻辑
|   ├── __init__.py
|   └── environment.py      # 包含PokerEnv类，我们项目的“官方赛场”
|
├── agents/                 # 存放所有AI智能体的实现
|   ├── __init__.py
|   ├── base_agent.py       # 定义所有Agent都必须遵守的“合同”
|   └── random_agent.py     # (测试用) 随机决策Agent
|
├── utils/                  # 存放数据处理等辅助工具
|   ├── __init__.py
|   ├── encoder.py          # 唯一的职责：将state字典翻译成PSV向量
|   └── logger.py           # 唯一的职责：记录人类可读和向量化的日志
|
├── logs/                   # (此文件夹由程序自动生成)
|
├── main.py                 # 程序的唯一入口，我们的“总导演”
|
├── docs/                   # 开发人员杂用文档
|
├── requirements.txt        
|
└── README.md               
```

## 3. 游戏规则

本平台将严格实现**单挑限注德州扑克 (HULHE)** 的标准规则。

- **游戏类型**: 1v1 对战。
- **核心参数**:
  - **初始总筹码**: 400
  - **小盲注 (SB)**: 1
  - **大盲注 (BB)**: 2
- **下注结构 (限注)**:
  - **小注 (Small Bet)**: 2 (用于翻牌前和翻牌圈)
  - **大注 (Big Bet)**: 4 (用于转牌和河牌圈)
  - **加注上限**: 每轮最多允许1次下注(Bet)和3次加注(Raise)。
- **行动顺序**:
  - **翻牌前 (Pre-flop)**: 庄家(Button/SB) **先行动**。
  - **翻牌后 (Flop, Turn, River)**: 大盲注(BB) **先行动**。
- **盲注轮换**:
  - 庄家位置在每局结束后必须轮换，以确保公平性和策略的完整性。
- **游戏模式**:
  - **随机化模拟模式 (Default)**: 每一局都是独立的实验。在每局开始时，系统会为一位玩家随机分配[10, 390]的筹码，另一位玩家则获得剩余的筹码（总和为400）。
  - **标准化评估模式 (Optional)**: 为了进行公平的模型性能评估，环境提供一个开关，可以将初始筹码固定为200/200。

## 4. 核心数据接口: PSV (Poker State Vector)

PSV是一个**301维**的`numpy`浮点数向量。它被设计为一个能**完整编码一局牌局所有信息的“基因序列”**。所有关于牌局动态演变的信息（包括筹码和底池变化）都完全包含在历史序列中，AI模型需要从这个序列的最后一个时间步来推断当前的数值状态。

| 组件 (Component) | 结构 (Structure) | 维度 (Dimension) | 备注 (Notes) |
| :--- | :--- | :--- | :--- |
| **1. 静态信息: 牌面与位置** | | **121** | |
| &nbsp;&nbsp;&nbsp; Private Hand | `2 * (13 + 4)` | `34` | 2张手牌，点数和花色one-hot编码。 |
| &nbsp;&nbsp;&nbsp; Community Cards | `5 * (13 + 4)` | `85` | 5张公共牌，编码方式同上。 |
| &nbsp;&nbsp;&nbsp; Position | `2` | `2` | 玩家位置（SB/BB），one-hot编码。 |
| **2. 动态信息: 富历史序列** | `4 * 5 * (3 + 2 + 4)` | **180** | 4轮*5步/轮*9维/步。9维 = 3维数值状态 + 2维玩家 + 4维动作。 |
| **总计 (Total)** | | **301** | |

## 5. AI训练与反馈机制 (初步规划)

### 5.1 奖励 (Reward)

- **奖励信号**: AI的奖励信号将是其在**单局牌局中的净筹码输赢**。
- **计算方式**: 由`HULHE_env/environment.py`中的`PokerEnv`在每局结束时精确计算 (`final_stack - initial_stack_this_hand`)，并通过`step`函数的`info`字典返回。
- **归一化**: 在未来的训练脚本中，这个原始的筹码输赢值将被**归一化**（例如，除以初始总筹码量400），以获得一个更稳定、范围更小的奖励信号（如-1到+1），这有利于神经网络的稳定训练。

### 5.2 训练时机

- **经验回放 (Experience Replay)**: 我们不会每打一局就训练一次。相反，我们会让AI先进行大量的自我对弈（例如10,000局），并将所有对局的`(PSV, result)`数据对存储在一个大型的“经验池”中。
- **批量训练 (Batch Training)**: 在训练阶段，我们将从经验池中**随机抽取**一小批数据（a batch, 例如256个样本）来对神经网络进行一次参数更新。这种方式可以打破数据间的相关性，并充分利用硬件性能，使训练更高效、更稳定。

### 5.3 AI模型设计与训练 (待详细规划)

> **开发者说明**: 此部分为未来工作的占位符。在核心环境100%稳定并通过所有测试后，我们将在此详细规划AI的具体模型架构、训练算法和迭代策略。

初步设想将遵循`README v1.0`中的监督学习到强化学习的路径：

1.  **阶段一：监督学习基准模型**
    *   **目标**: 训练一个能模仿基本策略的初始模型。
    *   **数据**: 使用`RandomAgent`生成的初始数据集。
    *   **模型**: 一个拥有“策略头”和“价值头”的双头神经网络。
    *   **训练**: 策略头学习模仿历史中的真实动作，价值头学习预测最终的（归一化）筹码输赢。

2.  **阶段二：强化学习自我进化**
    *   **目标**: 让AI通过自我博弈，超越初始数据，发现更优策略。
    *   **算法**: 探索如PPO (Proximal Policy Optimization) 或基于蒙特卡洛树搜索 (MCTS) 的方法。
    *   **循环**: **对弈 -> 存储经验 -> 抽样训练 -> 更新模型 -> 对弈...**

---